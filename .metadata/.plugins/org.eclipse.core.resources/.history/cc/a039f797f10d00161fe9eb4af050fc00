import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.util.StringTokenizer;
import java.util.Vector;

public class LDAModel {

	/*
	 * Model for LDA
	 * Contains all relevant information about the current run
	 */
	
	// store the data object => contains dictionary
	public LDAData data;
	
	// No.of Documents
	public int M;
	// Vocabulary Size
	public int V;
	// Number of topics
	public int K;
	
	//hyper-parameters for smoothing
	public double alpha, beta;
	
	//number of iterations to be run
	public int numIters;
	
	// lda parameters to be estimated
	// theta - (document topic distribution : size M * K)
	public double[][] theta;
	// phi - (topic word distribution : size K * V)
	public double[][] phi;


	// the main model variables used for each gibbs sampling
	public Vector<Integer>[] currentTopicAss;
	// nw[i][j] : size V * K : denotes number of times word i is assigned topic j 
	public int[][] nw;
	// nd[i][j] : size M * K : denotes number of words in document i assigned to topic j
	public int[][] nd;
	// nwSum[j] : size K : total number of words assigned to topic j
	public int[] nwSum;
	// ndSum : size M : total number of words in document
	public int[] ndSum;
	
	// contains probabilities used for sampling topic to word
	public double[] samplingProb;
	
	//constructor for setting default values
	public LDAModel(){
		
		setDefaultValues();
	}
	
	//customized model setting => prefer the above function
	public LDAModel(int K, double alpha, double beta, int numIters){
		
		setDefaultValues();
		
		this.K = K;
		this.alpha = alpha;
		this.beta = beta;
		this.numIters = numIters;
		
	}
	
	//sets the default values for the model
	public void setDefaultValues(){
		
		M = 0;
		V = 0;
		K = 100;
		alpha = 50.0/K;
		beta = 0.1;
		numIters = 2000;
		
		currentTopicAss = null;
		nw = null;
		nd = null;
		nwSum = null;
		ndSum = null;
		
		theta = null;
		phi = null;
		
	}
	
	//read file to get parameters
	public boolean setParamsFromFile(String configFile){
		
		BufferedReader br;
		try {
			br = new BufferedReader(new FileReader(configFile));
			String line;
			while( (line = br.readLine()) != null){
				
				StringTokenizer tokenizer = new StringTokenizer(line , "= \t\r\n");
				
				int noOfTokens = tokenizer.countTokens();
				if(noOfTokens != 2){
					continue;
				}
				
				String option = tokenizer.nextToken();
				String value = tokenizer.nextToken();
				
				if( option.equalsIgnoreCase("alpha")){
					alpha = Double.parseDouble(value);
				}
				if( option.equalsIgnoreCase("beta")){
					beta = Double.parseDouble(value);
				}
				if( option.equalsIgnoreCase("topics")){
					K = Integer.parseInt(value);
				}
				if( option.equalsIgnoreCase("iters")){
					numIters = Integer.parseInt(value);
				}
				
			}
			br.close();
			
		} catch (FileNotFoundException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			return false;
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			return false;
		}
		
		return true;
	}
	
	//load the ldaData
	public void loadModel(){
		
		/* This call read all the input files and creates dictionary
			else if already read, returns the read data
		*/
		data = new LDAData();
	}
	
	//once the model is loaded, it initializes the parameters of the model
	public void initModel(){
		
		//loads the model
		loadModel();
		M = data.M;
		V = data.V;
		
		// value of K taken from configFile or default value
		int wordIter,topicIter,docIter;
		
		nw = new int[V][K];
		for(wordIter = 0; wordIter < V; wordIter++){
			for(topicIter = 0; topicIter < K; topicIter++){
				nw[wordIter][topicIter] = 0;
			}
		}
		
		nd = new int[M][K];
		for(docIter = 0; docIter < M; docIter++){
			for(topicIter = 0; topicIter < K; topicIter++){
				nd[docIter][topicIter] = 0;
			}
		}
		
		nwSum = new int[K];
		for(topicIter = 0; topicIter < K; topicIter++){
			ndSum[topicIter] = 0;
		}
		
		ndSum = new int[M];
		for(docIter = 0; docIter < M; docIter++){
			ndSum[docIter] = 0;
		}
		
		//used to store all the current topic assignments
		currentTopicAss = new Vector[M];
		int docLength,n,topic;
		
		for(docIter = 0; docIter < M; docIter++){
			
			//read each document length and create a vector
			docLength = data.docs[docIter].length;
			currentTopicAss[docIter] = new Vector<Integer>();
			
			//initialize by a random topic assignment
			for(n=0; n< docLength; n++){
				
				//add randomly sampled point
				topic = (int)Math.floor(Math.random()*K);
				currentTopicAss[docIter].add(topic);
				
				//number of instances of word assigned to topic j
				nw[data.docs[docIter].words[n]][topic] += 1;
				//number of words in document i assigned top topic j
				nd[docIter][topic] += 1;
				//total number of words assigned to topic j
				nwSum[topic] += 1;
			}
			//total number of words in ith document
			ndSum[docIter] = docLength;
			
		}
		
		theta = new double[M][K];
		phi = new double[K][V];
	}
}
